# ===================== Train EfficientNet-B0 on THFOOD-50 =====================
import os, contextlib
from pathlib import Path

import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import models, transforms, datasets
from torchvision.models import EfficientNet_B0_Weights
from PIL import ImageFile
from tqdm import tqdm

ImageFile.LOAD_TRUNCATED_IMAGES = True

# ----------------- Config -----------------
ROOT = Path("D:/thai-food-ai")
DATASET = "THFOOD-50"
train_dir = ROOT / DATASET / "train"
val_dir   = ROOT / DATASET / "val"

IMG_SIZE = 224
BATCH_TRAIN = 16
BATCH_VAL   = 32
EPOCHS_S1   = 10
EPOCHS_S2   = 60
USE_AMP     = True
LABEL_SMOOTH = 0.05
WEIGHT_DECAY = 2e-4
NUM_CLASSES = 50

BEST_MODEL_NAME = "best_model_thfood50.pth"
CLASS_MAP_CSV   = "class_map_thfood50.csv"
CKPT_NAME       = "checkpoint_thfood50.pth"

torch.backends.cudnn.benchmark = True   # à¹ƒà¸Šà¹‰ cudnn.benchmark à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰à¹€à¸¥à¸·à¸­à¸ kernel à¸—à¸µà¹ˆà¹€à¸£à¹‡à¸§à¸—à¸µà¹ˆà¸ªà¸¸à¸”à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

# ----------------- AMP helper -----------------
try:
    from torch.amp import GradScaler, autocast
    _AMP_IS_NEW = True
except Exception:
    from torch.cuda.amp import GradScaler, autocast
    _AMP_IS_NEW = False

def make_scaler():
    if not (USE_AMP and device.type == "cuda"):
        return None
    try:
        return GradScaler(device_type="cuda")  # AMP GradScaler: à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸›à¸±à¸à¸«à¸² gradient underflow à¹€à¸¡à¸·à¹ˆà¸­à¹ƒà¸Šà¹‰ FP16
    except TypeError:
        return GradScaler()

def amp_cast(enabled: bool):
    if not enabled:
        return contextlib.nullcontext()
    if _AMP_IS_NEW:
        try:
            return autocast(device_type="cuda", enabled=True)  # AMP autocast: à¹ƒà¸Šà¹‰ precision à¹à¸šà¸šà¸œà¸ªà¸¡ (FP16/FP32) à¹€à¸à¸·à¹ˆà¸­à¹ƒà¸«à¹‰ train à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™
        except TypeError:
            return autocast(enabled=True)
    else:
        return autocast(enabled=True)

# ----------------- Transforms -----------------
mean=[0.485,0.456,0.406]; std=[0.229,0.224,0.225]

train_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),       # Data augmentation: à¸à¸¥à¸´à¸à¸ à¸²à¸à¹à¸™à¸§à¸™à¸­à¸™
    transforms.RandomVerticalFlip(p=0.1),    # Data augmentation: à¸à¸¥à¸´à¸à¸ à¸²à¸à¹à¸™à¸§à¸•à¸±à¹‰à¸‡à¹€à¸¥à¹‡à¸à¸™à¹‰à¸­à¸¢
    transforms.RandomRotation(30),           # Data augmentation: à¸«à¸¡à¸¸à¸™à¸ à¸²à¸
    transforms.ColorJitter(brightness=0.3),  # Data augmentation: à¸›à¸£à¸±à¸šà¸„à¸§à¸²à¸¡à¸ªà¸§à¹ˆà¸²à¸‡
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # Data augmentation: à¸„à¸£à¸­à¸›à¹à¸šà¸šà¸ªà¸¸à¹ˆà¸¡
    transforms.ToTensor(),
    transforms.Normalize(mean, std),         # Normalize à¸”à¹‰à¸§à¸¢à¸„à¹ˆà¸² mean/std à¸‚à¸­à¸‡ ImageNet
])
val_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

# ----------------- Dataset / Loader -----------------
train_ds = datasets.ImageFolder(train_dir, transform=train_tf)
val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)

if len(train_ds.classes) != NUM_CLASSES:
    print(f"âš ï¸ Warning: à¸à¸š {len(train_ds.classes)} à¸„à¸¥à¸²à¸ªà¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ (à¸„à¸²à¸”à¸§à¹ˆà¸² 50)")

train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,
                          num_workers=0, pin_memory=True, persistent_workers=False)  # à¹ƒà¸Šà¹‰ pin_memory à¹€à¸à¸·à¹ˆà¸­à¹€à¸£à¹ˆà¸‡à¸à¸²à¸£ copy à¹„à¸›à¸¢à¸±à¸‡ GPU
val_loader   = DataLoader(val_ds,   batch_size=BATCH_VAL,   shuffle=False,
                          num_workers=0, pin_memory=True, persistent_workers=False)

# ----------------- Model -----------------
weights = EfficientNet_B0_Weights.IMAGENET1K_V1
model = models.efficientnet_b0(weights=weights)  # EfficientNet-B0 à¹ƒà¸Šà¹‰à¹€à¸—à¸„à¸™à¸´à¸„: MBConv, Depthwise Separable Conv, SE Block, Swish(SiLU)

# à¹à¸à¹‰ classifier à¸ªà¸¸à¸”à¸—à¹‰à¸²à¸¢à¹ƒà¸«à¹‰à¸•à¸£à¸‡à¸à¸±à¸šà¸ˆà¸³à¸™à¸§à¸™à¸„à¸¥à¸²à¸ªà¹ƒà¸«à¸¡à¹ˆ
model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)  # Classifier à¸¡à¸µ Dropout à¸ à¸²à¸¢à¹ƒà¸™ â†’ à¸Šà¹ˆà¸§à¸¢à¸¥à¸” overfitting

model = model.to(device).to(memory_format=torch.channels_last)  # à¹ƒà¸Šà¹‰ memory_format=channels_last à¹€à¸à¸·à¹ˆà¸­à¹€à¸£à¹ˆà¸‡à¸šà¸™ Tensor Core

# ----------------- Loss / Optimizer / Scheduler -----------------
criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)  
# â†‘ Label smoothing: à¸à¸£à¸°à¸ˆà¸²à¸¢à¸„à¸§à¸²à¸¡à¸™à¹ˆà¸²à¸ˆà¸°à¹€à¸›à¹‡à¸™à¸‚à¸­à¸‡ target â†’ à¸¥à¸”à¸„à¸§à¸²à¸¡à¸¡à¸±à¹ˆà¸™à¹ƒà¸ˆà¹€à¸à¸´à¸™à¹„à¸›à¸‚à¸­à¸‡à¹‚à¸¡à¹€à¸”à¸¥ à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ overfitting

optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY)  
# â†‘ AdamW: Optimizer à¸—à¸µà¹ˆ decouple weight decay à¸Šà¹ˆà¸§à¸¢ regularization à¸”à¸µà¸à¸§à¹ˆà¸² Adam+L2

scaler = make_scaler()  # à¹ƒà¸Šà¹‰ AMP GradScaler (mixed precision training)

from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=3, min_lr=1e-6)  
# â†‘ ReduceLROnPlateau: à¸¥à¸” learning rate à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´à¹€à¸¡à¸·à¹ˆà¸­ val_acc à¹„à¸¡à¹ˆà¸”à¸µà¸‚à¸¶à¹‰à¸™

# ----------------- Utils -----------------
def validate():
    model.eval()
    correct, total, vloss = 0, 0, 0.0
    with torch.no_grad():  # à¸›à¸´à¸” gradient â†’ à¸›à¸£à¸°à¹€à¸¡à¸´à¸™à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™/à¸›à¸£à¸°à¸«à¸¢à¸±à¸”à¹à¸£à¸¡
        for x, y in val_loader:
            x = x.to(device, non_blocking=True)  # à¹ƒà¸Šà¹‰ non_blocking copy à¹€à¸à¸·à¹ˆà¸­à¹€à¸£à¹ˆà¸‡à¸à¸²à¸£à¸ªà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹„à¸› GPU
            y = y.to(device, non_blocking=True)
            with amp_cast(scaler is not None):   # à¹ƒà¸Šà¹‰ AMP à¹ƒà¸™à¸à¸²à¸£ inference à¹€à¸Šà¹ˆà¸™à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸š training
                out = model(x)
                loss = criterion(out, y)
            vloss += loss.item() * x.size(0)
            pred = out.argmax(1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    return correct / total, vloss / total

# ----------------- Train -----------------
if __name__ == "__main__":
    try:
        # Stage 1: Freeze backbone â†’ à¸à¸¶à¸à¹€à¸‰à¸à¸²à¸° classifier à¸à¹ˆà¸­à¸™ (Fine-tuning step à¹à¸£à¸)
        for p in model.features.parameters():
            p.requires_grad = False

        for ep in range(EPOCHS_S1):
            model.train()
            correct = 0
            for x, y in tqdm(train_loader, desc=f"S1 {ep+1}/{EPOCHS_S1}"):
                x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)  # set_to_none=True à¸Šà¹ˆà¸§à¸¢à¸¥à¸” memory fragmentation
                with amp_cast(scaler is not None):     # AMP mixed precision
                    out = model(x); loss = criterion(out, y)
                if scaler:
                    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
                else:
                    loss.backward(); optimizer.step()
                correct += (out.argmax(1) == y).sum().item()

            train_acc = correct / len(train_ds)
            val_acc, vloss = validate()
            scheduler.step(val_acc)  # ReduceLROnPlateau à¸›à¸£à¸±à¸š LR
            print(f"[S1 {ep+1}] train_acc={train_acc:.4f} | val_acc={val_acc:.4f} | val_loss={vloss:.4f}")

        # Stage 2: Unfreeze ~70% à¸‚à¸­à¸‡ backbone â†’ Fine-tune à¸¥à¸¶à¸à¸‚à¸¶à¹‰à¸™
        feat = list(model.features)
        freeze_until = int(len(feat) * 0.3)
        for i, layer in enumerate(feat):
            for p in layer.parameters():
                p.requires_grad = i >= freeze_until

        # à¹ƒà¸Šà¹‰ AdamW à¸­à¸µà¸à¸„à¸£à¸±à¹‰à¸‡ à¹à¸•à¹ˆà¸¥à¸” learning rate à¸¥à¸‡à¹€à¸à¸·à¹ˆà¸­ fine-tune à¸­à¸¢à¹ˆà¸²à¸‡à¸£à¸°à¸¡à¸±à¸”à¸£à¸°à¸§à¸±à¸‡
        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                                lr=1e-4, weight_decay=WEIGHT_DECAY)

        best, patience, wait = 0.0, 10, 0  # à¹ƒà¸Šà¹‰ Early stopping à¸”à¹‰à¸§à¸¢ patience=10
        for ep in range(EPOCHS_S2):
            model.train()
            correct = 0
            for x, y in tqdm(train_loader, desc=f"S2 {ep+1}/{EPOCHS_S2}"):
                x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)
                with amp_cast(scaler is not None):
                    out = model(x); loss = criterion(out, y)
                if scaler:
                    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
                else:
                    loss.backward(); optimizer.step()
                correct += (out.argmax(1) == y).sum().item()

            train_acc = correct / len(train_ds)
            val_acc, vloss = validate()
            scheduler.step(val_acc)  # ReduceLROnPlateau à¸›à¸£à¸±à¸š LR à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´
            print(f"[S2 {ep+1}] train_acc={train_acc:.4f} | val_acc={val_acc:.4f} | val_loss={vloss:.4f}")

            if val_acc > best:
                best = val_acc; wait = 0
                torch.save(model.state_dict(), ROOT / BEST_MODEL_NAME)  # Save best model
                with open(ROOT / CLASS_MAP_CSV, "w", encoding="utf-8") as f:
                    for idx, name in enumerate(train_ds.classes):
                        f.write(f"{idx},{name},{name}\n")
                print(f"ğŸ’¾ saved: {BEST_MODEL_NAME}, {CLASS_MAP_CSV}")
            else:
                wait += 1
                if wait >= patience:
                    print("â¹ï¸ early stop"); break  # Early stopping: à¸«à¸¢à¸¸à¸” train à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸”à¸µà¸‚à¸¶à¹‰à¸™

        print(f"âœ… DONE. Best val_acc={best:.4f}")

    except KeyboardInterrupt:
        torch.save(model.state_dict(), ROOT / CKPT_NAME)  # Save checkpoint à¸à¸£à¸“à¸µà¸«à¸¢à¸¸à¸”à¹€à¸­à¸‡
        print(f"ğŸ’¾ Saved {CKPT_NAME} (interrupted)")
        raise
