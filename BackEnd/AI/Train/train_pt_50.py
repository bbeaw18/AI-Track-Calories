# ===================== Train EfficientNet-B0 on THFOOD-50 =====================
import os, contextlib
from pathlib import Path

import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from torchvision import models, transforms, datasets
from torchvision.models import EfficientNet_B0_Weights
from PIL import ImageFile
from tqdm import tqdm

ImageFile.LOAD_TRUNCATED_IMAGES = True

# ----------------- Config -----------------
ROOT = Path("D:/thai-food-ai")
DATASET = "THFOOD-50"
train_dir = ROOT / DATASET / "train"
val_dir   = ROOT / DATASET / "val"

IMG_SIZE = 224
BATCH_TRAIN = 16
BATCH_VAL   = 32
EPOCHS_S1   = 10
EPOCHS_S2   = 60
USE_AMP     = True
LABEL_SMOOTH = 0.05
WEIGHT_DECAY = 2e-4
NUM_CLASSES = 50

BEST_MODEL_NAME = "best_model_thfood50.pth"
CLASS_MAP_CSV   = "class_map_thfood50.csv"
CKPT_NAME       = "checkpoint_thfood50.pth"

torch.backends.cudnn.benchmark = True   # ‡πÉ‡∏ä‡πâ cudnn.benchmark ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å kernel ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ Using device: {device}")

# ----------------- AMP helper -----------------
try:
    from torch.amp import GradScaler, autocast
    _AMP_IS_NEW = True
except Exception:
    from torch.cuda.amp import GradScaler, autocast
    _AMP_IS_NEW = False

def make_scaler():
    if not (USE_AMP and device.type == "cuda"):
        return None
    try:
        return GradScaler(device_type="cuda")  # AMP GradScaler: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ gradient underflow ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ FP16
    except TypeError:
        return GradScaler()

def amp_cast(enabled: bool):
    if not enabled:
        return contextlib.nullcontext()
    if _AMP_IS_NEW:
        try:
            return autocast(device_type="cuda", enabled=True)  # AMP autocast: ‡πÉ‡∏ä‡πâ precision ‡πÅ‡∏ö‡∏ö‡∏ú‡∏™‡∏° (FP16/FP32) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ train ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
        except TypeError:
            return autocast(enabled=True)
    else:
        return autocast(enabled=True)

# ----------------- Transforms -----------------
mean=[0.485,0.456,0.406]; std=[0.229,0.224,0.225]

train_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),       # Data augmentation: ‡∏û‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô
    transforms.RandomVerticalFlip(p=0.1),    # Data augmentation: ‡∏û‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏ô‡∏ß‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
    transforms.RandomRotation(30),           # Data augmentation: ‡∏´‡∏°‡∏∏‡∏ô‡∏†‡∏≤‡∏û
    transforms.ColorJitter(brightness=0.3),  # Data augmentation: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á
    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),  # Data augmentation: ‡∏Ñ‡∏£‡∏≠‡∏õ‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°
    transforms.ToTensor(),
    transforms.Normalize(mean, std),         # Normalize ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤ mean/std ‡∏Ç‡∏≠‡∏á ImageNet
])
val_tf = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

# ----------------- Dataset / Loader -----------------
train_ds = datasets.ImageFolder(train_dir, transform=train_tf)
val_ds   = datasets.ImageFolder(val_dir,   transform=val_tf)

if len(train_ds.classes) != NUM_CLASSES:
    print(f"‚ö†Ô∏è Warning: ‡∏û‡∏ö {len(train_ds.classes)} ‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå (‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤ 50)")

train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN, shuffle=True,
                          num_workers=0, pin_memory=True, persistent_workers=False)  # ‡πÉ‡∏ä‡πâ pin_memory ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Å‡∏≤‡∏£ copy ‡πÑ‡∏õ‡∏¢‡∏±‡∏á GPU
val_loader   = DataLoader(val_ds,   batch_size=BATCH_VAL,   shuffle=False,
                          num_workers=0, pin_memory=True, persistent_workers=False)

# ----------------- Model -----------------
weights = EfficientNet_B0_Weights.IMAGENET1K_V1
model = models.efficientnet_b0(weights=weights)  # EfficientNet-B0 ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: MBConv, Depthwise Separable Conv, SE Block, Swish(SiLU)

# ‡πÅ‡∏Å‡πâ classifier ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏´‡∏°‡πà
model.classifier[1] = nn.Linear(model.classifier[1].in_features, NUM_CLASSES)  # Classifier ‡∏°‡∏µ Dropout ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô ‚Üí ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î overfitting

model = model.to(device).to(memory_format=torch.channels_last)  # ‡πÉ‡∏ä‡πâ memory_format=channels_last ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏ö‡∏ô Tensor Core

# ----------------- Loss / Optimizer / Scheduler -----------------
criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)  
# ‚Üë Label smoothing: ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á target ‚Üí ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting

optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY)  
# ‚Üë AdamW: Optimizer ‡∏ó‡∏µ‡πà decouple weight decay ‡∏ä‡πà‡∏ß‡∏¢ regularization ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Adam+L2

scaler = make_scaler()  # ‡πÉ‡∏ä‡πâ AMP GradScaler (mixed precision training)

from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode="max", factor=0.5, patience=3, min_lr=1e-6)  
# ‚Üë ReduceLROnPlateau: ‡∏•‡∏î learning rate ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠ val_acc ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

# ----------------- Utils -----------------
def validate():
    model.eval()
    correct, total, vloss = 0, 0, 0.0
    with torch.no_grad():  # ‡∏õ‡∏¥‡∏î gradient ‚Üí ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô/‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏£‡∏°
        for x, y in val_loader:
            x = x.to(device, non_blocking=True)  # ‡πÉ‡∏ä‡πâ non_blocking copy ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ GPU
            y = y.to(device, non_blocking=True)
            with amp_cast(scaler is not None):   # ‡πÉ‡∏ä‡πâ AMP ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ inference ‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö training
                out = model(x)
                loss = criterion(out, y)
            vloss += loss.item() * x.size(0)
            pred = out.argmax(1)
            correct += (pred == y).sum().item()
            total += y.size(0)
    return correct / total, vloss / total

# ----------------- Train -----------------
if __name__ == "__main__":
    try:
        # Stage 1: Freeze backbone ‚Üí ‡∏ù‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ classifier ‡∏Å‡πà‡∏≠‡∏ô (Fine-tuning step ‡πÅ‡∏£‡∏Å)
        for p in model.features.parameters():
            p.requires_grad = False

        for ep in range(EPOCHS_S1):
            model.train()
            correct = 0
            for x, y in tqdm(train_loader, desc=f"S1 {ep+1}/{EPOCHS_S1}"):
                x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)  # set_to_none=True ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î memory fragmentation
                with amp_cast(scaler is not None):     # AMP mixed precision
                    out = model(x); loss = criterion(out, y)
                if scaler:
                    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
                else:
                    loss.backward(); optimizer.step()
                correct += (out.argmax(1) == y).sum().item()

            train_acc = correct / len(train_ds)
            val_acc, vloss = validate()
            scheduler.step(val_acc)  # ReduceLROnPlateau ‡∏õ‡∏£‡∏±‡∏ö LR
            print(f"[S1 {ep+1}] train_acc={train_acc:.4f} | val_acc={val_acc:.4f} | val_loss={vloss:.4f}")

        # Stage 2: Unfreeze ~70% ‡∏Ç‡∏≠‡∏á backbone ‚Üí Fine-tune ‡∏•‡∏∂‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        feat = list(model.features)
        freeze_until = int(len(feat) * 0.3)
        for i, layer in enumerate(feat):
            for p in layer.parameters():
                p.requires_grad = i >= freeze_until

        # ‡πÉ‡∏ä‡πâ AdamW ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡πÅ‡∏ï‡πà‡∏•‡∏î learning rate ‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠ fine-tune ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á
        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                                lr=1e-4, weight_decay=WEIGHT_DECAY)

        best, patience, wait = 0.0, 10, 0  # ‡πÉ‡∏ä‡πâ Early stopping ‡∏î‡πâ‡∏ß‡∏¢ patience=10
        for ep in range(EPOCHS_S2):
            model.train()
            correct = 0
            for x, y in tqdm(train_loader, desc=f"S2 {ep+1}/{EPOCHS_S2}"):
                x = x.to(device, non_blocking=True); y = y.to(device, non_blocking=True)
                optimizer.zero_grad(set_to_none=True)
                with amp_cast(scaler is not None):
                    out = model(x); loss = criterion(out, y)
                if scaler:
                    scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
                else:
                    loss.backward(); optimizer.step()
                correct += (out.argmax(1) == y).sum().item()

            train_acc = correct / len(train_ds)
            val_acc, vloss = validate()
            scheduler.step(val_acc)  # ReduceLROnPlateau ‡∏õ‡∏£‡∏±‡∏ö LR ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
            print(f"[S2 {ep+1}] train_acc={train_acc:.4f} | val_acc={val_acc:.4f} | val_loss={vloss:.4f}")

            if val_acc > best:
                best = val_acc; wait = 0
                torch.save(model.state_dict(), ROOT / BEST_MODEL_NAME)  # Save best model
                with open(ROOT / CLASS_MAP_CSV, "w", encoding="utf-8") as f:
                    for idx, name in enumerate(train_ds.classes):
                        f.write(f"{idx},{name},{name}\n")
                print(f"üíæ saved: {BEST_MODEL_NAME}, {CLASS_MAP_CSV}")
            else:
                wait += 1
                if wait >= patience:
                    print("‚èπÔ∏è early stop"); break  # Early stopping: ‡∏´‡∏¢‡∏∏‡∏î train ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô

        print(f"‚úÖ DONE. Best val_acc={best:.4f}")

    except KeyboardInterrupt:
        torch.save(model.state_dict(), ROOT / CKPT_NAME)  # Save checkpoint ‡∏Å‡∏£‡∏ì‡∏µ‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏≠‡∏á
        print(f"üíæ Saved {CKPT_NAME} (interrupted)")
        raise
